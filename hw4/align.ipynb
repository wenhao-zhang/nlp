{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Word Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group: Wisefish \n",
    "\n",
    "* Wenhao Zhang, wenhaoz \n",
    "* Graeme Milne, gmilne \n",
    "* Mitchell McCormack, mmccorma\n",
    "* Jonathan Lo, jcl60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development process\n",
    "\n",
    "As with previous assignments, we arranged a group meeting in which we would all read the assignment outline and came with questions and points to discuss. We planned a few hours long meeting in order to begin implementing the algorithm. \n",
    "\n",
    "The one session ended with a nearly complete implementation of the baseline algorithm. However, there were some remaining issues we found while debugging the baseline at large values of N that still required work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Command line setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import optparse, sys, os, logging\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional command line parameter `-m` was added to specify which model to use for training. `-m m1` will train using only model 1 and `-m m2` will train using model 2. `m2` is the default setting for the command line parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optparser = optparse.OptionParser()\n",
    "optparser.add_option(\"-d\", \"--datadir\", dest=\"datadir\", default=\"data\", help=\"data directory (default=data)\")\n",
    "optparser.add_option(\"-p\", \"--prefix\", dest=\"fileprefix\", default=\"hansards\", help=\"prefix of parallel data files (default=hansards)\")\n",
    "optparser.add_option(\"-e\", \"--english\", dest=\"english\", default=\"en\", help=\"suffix of English (target language) filename (default=en)\")\n",
    "optparser.add_option(\"-f\", \"--french\", dest=\"french\", default=\"fr\", help=\"suffix of French (source language) filename (default=fr)\")\n",
    "optparser.add_option(\"-l\", \"--logfile\", dest=\"logfile\", default=None, help=\"filename for logging output\")\n",
    "optparser.add_option(\"-t\", \"--threshold\", dest=\"threshold\", default=0.5, type=\"float\", help=\"threshold for alignment (default=0.5)\")\n",
    "optparser.add_option(\"-n\", \"--num_sentences\", dest=\"num_sents\", default=sys.maxsize, type=\"int\", help=\"Number of sentences to use for training and alignment\")\n",
    "optparser.add_option(\"-m\", \"--model\", dest=\"model\", default = \"m2\", help=\"m1 for model 1 and m2 for model 2\")\n",
    "(opts, _) = optparser.parse_args()\n",
    "\n",
    "f_data = \"%s.%s\" % (os.path.join(opts.datadir, opts.fileprefix), opts.french)\n",
    "e_data = \"%s.%s\" % (os.path.join(opts.datadir, opts.fileprefix), opts.english)\n",
    "\n",
    "\n",
    "if opts.logfile:\n",
    "    logging.basicConfig(filename=opts.logfile, filemode='w', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 1 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The running probability of t(f<sub>i</sub>, e<sub>j</sub>) is initialized as a dictionary of dictionaries and all sets of dictionaries are defaultdicts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bitext, f_vocab, e_vocab = loadData(f_data, e_data, opts.num_sents)\n",
    "size_f_vocab = len(f_vocab)\n",
    "t = defaultdict(lambda: defaultdict(lambda: 1/size_f_vocab))\n",
    "\n",
    "def loadData(f_data, e_data, num_sents):\n",
    "  bitext = [[sentence.strip().split() for sentence in pair] for pair in islice(zip(open(f_data), open(e_data)), num_sents)]\n",
    "  f_vocab = set()\n",
    "  e_vocab = set()\n",
    "\n",
    "  for s in islice(open(f_data), num_sents):\n",
    "      words = s.strip().split()\n",
    "      for w in words:\n",
    "          f_vocab.add(w)\n",
    "\n",
    "  for s in islice(open(e_data), num_sents):\n",
    "      words = s.strip().split()\n",
    "      for w in words:\n",
    "          e_vocab.add(w)\n",
    "\n",
    "  return bitext, f_vocab, e_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our initial implementation which used tuples of `(f_i, e_i)`, we encountered memory errors from `t` (as well as `fe_count` in each training stage) growing too large to be held in memory. Even with in a VM with more memory than in most computers, we could not get a full training run complete with `n=100000`.\n",
    "\n",
    "The root cause of this is due to the behaviour of Python's `defaultdict`. When an access is made to an unknown key, the `defaultdict` will set the default value (from evaluating the parameter from creation) and return it to whatever made the request. This resulted in a huge amount of key value pairs: keys from every single query, values which most of the time were just the default value.\n",
    "\n",
    "What we implemented was a lookup helper for `t` (and `a` for model 2). Using `if ... in ...` to check for key existance in a `defaultdict` does NOT set the default value into it, so we can save a lot of memory by simply checking for the existance of the key, otherwise returning the default value as a simple value.\n",
    "\n",
    "We also found out that using tuples as keys also had a significant impact on memory usage (and runtime to a small extent). So we switched to using a dict of dicts, with `f_i` values as the first key and `e_i` values as the second key.\n",
    "The structure of dicts in a dict halved the size of required memory for `t`, and so we were able to achieve a memory usage level which allowed us to run this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup(f_i, e_j):\n",
    "    if f_i in t:\n",
    "        if e_j in t[f_i]:\n",
    "            return t[f_i][e_j]\n",
    "    return 1 / size_f_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a lexical word alignment model\n",
    "\n",
    "Training the Model is done by iteratively improving the conditional probability for the translation. In order to do this, we go over the entire corpus' sentences. We compute the expected counts of all possible alignments and the total number of times we see a particular alignment with a foreign word. After computing the expected counts over all the sentences, we update our probabilities and continue this process until convergence. In our case the there is no convergence calculation at the end of each iteration, instead we settled on using a more simplistic and practical training of 5 iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    e_count = defaultdict(int)\n",
    "    fe_count = defaultdict(lambda: defaultdict(int))\n",
    "    for (n, (f, e)) in enumerate(bitext):\n",
    "        e = [\"NULL\"] + e\n",
    "        for (i, f_i) in enumerate(f):\n",
    "            Z = 0\n",
    "            for (j, e_j) in enumerate(e):\n",
    "                Z += lookup(f_i, e_j)\n",
    "            for (j, e_j) in enumerate(e):\n",
    "                c = lookup(f_i, e_j)/Z\n",
    "                fe_count[f_i][e_j] += c\n",
    "                e_count[e_j] += c\n",
    "    for f_i,e_is in fe_count.items():\n",
    "        for e_i, value in e_is.items():\n",
    "            t[f_i][e_i] = value/ e_count[e_i]\n",
    "return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Decoding the best alignment\n",
    "\n",
    "The final step of the process is the actual alignment of our data given the model we have just trained. This is done by iterating through to find the arg max alignment for any given translation pair. Or, as written in the assignment: \n",
    "\n",
    "<b> \n",
    "\\begin{align}\n",
    "\\hat{\\textbf{a}} = \\arg\\max_{\\textbf{a}} \\Pr(\\textbf{a} \\mid \\textbf{e}, \\textbf{f})\n",
    "\\end{align}\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (n, (f, e)) in enumerate(bitext):\n",
    "    current_alignment = \"\"\n",
    "    for (i, f_i) in enumerate(f): \n",
    "        bestp = 0\n",
    "        bestj = 0\n",
    "        for (j, e_j) in enumerate(e):\n",
    "            if t[(f_i,e_j)] > bestp:\n",
    "                bestp = t[(f_i,e_j)]\n",
    "                bestj = j\n",
    "        sys.stdout.write(\"%i-%i \" % (i,bestj))\n",
    "    sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an alignment from the German-English Europarl dataset (Model 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! python3 align.py -p europarl -f de -n 100000 -m m1 > output.a \n",
    "! head -1000 output.a > europarl_align.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and score an alignment on the French-English Hansard dataset (Model 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! python3 align.py -n 100000 -m m1| python3 score-alignments.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements with a Model 2 Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization for Model 2 involves running Model 1 to train the translation probabilities. This means we first uniformly initialize the translation probability as before but instead of using it directly in Model 2, we use Model 1 to train it first. Model 2 adds in the reverse distortion probability to the model. That is a(a<sub>i</sub>| i, I, J). We initialize the dictionary for a uniformly. The lookup function returns the value 1/(m+1) as the initial value for each value of a. This uniform distribution assumes all positions for a translation is equally likely for all m+1 positions. It is m+1 because it includes the Null alignment.\n",
    "\n",
    "The alignment probablilty for model 2 is now: P(__f , a__ | __e__) = $\\prod_{i=1}^{I}$ t(f<sub>i</sub>, e<sub>a<sub>j</sub></sub>) $\\times$ a(a<sub>i</sub>| i, I, J) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitext, f_vocab, e_vocab = loadData(f_data, e_data, opts.num_sents)\n",
    "size_f_vocab = len(f_vocab)\n",
    "\n",
    "model_one__trained_t = train_model_one(bitext, t, t_lookup)\n",
    "a = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 1/opts.num_sents))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The look up functions provide coverage for any case where the value of `t(f_i, e_j)` or `a(f_i,e_j)` is 0. Since these values are assigned to Z and Z is a denominator in the algorithm, they cannot be 0. \n",
    "\n",
    "As mentioned above for Model 1, the lookup functions also prevents the default dictionaries from becoming to bloated in memory by returning the base values of `1 / size_f_vocab` (for `t`) and `1/(m+1)` (for `a`) meaning we do not need to store the base values in the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def t_lookup(f_i, e_j):\n",
    "    if f_i in t:\n",
    "        if e_j in t[f_i]:\n",
    "            return t[f_i][e_j]\n",
    "    return 1 / size_f_vocab\n",
    "\n",
    "def a_lookup(i,j,l,m):\n",
    "    if i in a:\n",
    "        if j in a[i]:\n",
    "            if l in a[i][j]:\n",
    "                if m in a[i][j][l]:\n",
    "                    return a[i][j][l][m]\n",
    "    return 1/(m+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 improves upon Model 1 alignment by taking into account distortions. Instead of using the forward distortion, we use the reverse distortion a(a<sub>i</sub>| i, I, J). This means we have to modify the normalization vector, Z. Z is modified to the product of `t[f_i][e_j]` and `a[i][j][l][m]` where `l` and `m` are the lengths of the given French and English sentences. The algorithm for Model 2 is otherwise the same as Model 1. It is a EM algorithm. We first collect the counts and then update t and a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    for iteration in range(5):\n",
    "        e_count = defaultdict(int)\n",
    "        fe_count = defaultdict(lambda: defaultdict(int))\n",
    "        ijlm_count = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(int))))\n",
    "        ilm_count = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "        for (n, (f, e)) in enumerate(bitext):\n",
    "            f_null = [\"PLACE HOLDER\"] + f\n",
    "            e_null = [\"NULL\"] + e \n",
    "            l = len(f)\n",
    "            m = len(e)\n",
    "            for i in range(1, l + 1):\n",
    "                Z = 0\n",
    "                f_i = f_null[i]\n",
    "                for j in range(0, m + 1):\n",
    "                    e_j = e_null[j]\n",
    "                    Z += t_lookup(f_i, e_j)*a_lookup(i,j,len(f),len(e))\n",
    "                for j in range(0, m + 1):\n",
    "                    e_j = e_null[j]\n",
    "                    c = (t_lookup(f_i, e_j)*a_lookup(i,j,len(f),len(e)))/Z\n",
    "                    fe_count[f_i][e_j] += c\n",
    "                    e_count[e_j] += c\n",
    "                    ijlm_count[i][j][len(f)][len(e)] += c\n",
    "                    ilm_count[i][len(f)][len(e)] += c\n",
    "        \n",
    "        for f_i,e_is in fe_count.items():\n",
    "            for e_j, value in e_is.items():\n",
    "                t[f_i][e_j] = value/ e_count[e_j]\n",
    "        \n",
    "        for i, js in ijlm_count.items():\n",
    "            for j,ls in js.items():\n",
    "                for l, ms in ls.items():\n",
    "                    for m, value in ms.items():\n",
    "                        a[i][j][l][m] = value/ilm_count[i][l][m]\n",
    "    return t,a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Decoding the best alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    for (n, (f, e)) in enumerate(bitext):\n",
    "        current_sentence = \"\"\n",
    "        l = len(f)\n",
    "        m = len(e)\n",
    "        for (i, f_i) in enumerate(f): \n",
    "            bestp = t_lookup(f_i, \"NULL\") * a_lookup(i + 1, 0, l, m)\n",
    "            bestj = -1\n",
    "            for (j, e_j) in enumerate(e):\n",
    "                prob = t_lookup(f_i, e_j) * a_lookup(i + 1,j + 1,l,m)\n",
    "                if prob > bestp:\n",
    "                    bestp = prob\n",
    "                    bestj = j\n",
    "            if bestj != -1:\n",
    "                current_sentence += \"%i-%i \" % (i ,bestj)\n",
    "        alignment.append(current_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an alignment from the German-English Europarl dataset (Model 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! python3 align.py -p europarl -f de -n 100000 -m m2 > output.a \n",
    "! head -1000 output.a > europarl_align.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and score an alignment on the French-English Hansard dataset (Model 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! python3 align.py -n 100000 -m m2 | python3 score-alignments.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
